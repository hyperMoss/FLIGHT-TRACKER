from bs4 import BeautifulSoup
from DrissionPage import WebPage, ChromiumOptions
from time import sleep
import time
import requests
import json
from datetime import datetime

def send_to_feishu(content, webhook_url):
    """å‘é€æ¶ˆæ¯åˆ°é£ä¹¦æœºå™¨äºº"""
    try:
        headers = {
            'Content-Type': 'application/json'
        }
        data = {
            "msg_type": "text",
            "content": {
                "text": content
            }
        }
        response = requests.post(webhook_url, headers=headers, data=json.dumps(data))
        if response.status_code == 200:
            print("âœ… æ¶ˆæ¯å·²æˆåŠŸå‘é€åˆ°é£ä¹¦")
        else:
            print(f"âŒ å‘é€åˆ°é£ä¹¦å¤±è´¥: {response.status_code}")
    except Exception as e:
        print(f"âŒ å‘é€é£ä¹¦æ¶ˆæ¯æ—¶å‡ºé”™: {e}")

def format_flights_for_feishu(flights):
    """æ ¼å¼åŒ–èˆªç­ä¿¡æ¯ä¸ºé£ä¹¦æ¶ˆæ¯"""
    current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    message = f"ğŸ›« æœ€ä½ä»·æ ¼èˆªç­ä¿¡æ¯ ({current_time})\n"
    message += "=" * 50 + "\n\n"
    
    for i, flight in enumerate(flights, 1):
        message += f"ğŸ† ç¬¬{i}å - ä»·æ ¼: {flight['price']}\n"
        message += f"âœˆï¸ èˆªç©ºå…¬å¸: {flight['airline']}\n"
        message += f"ğŸ›« å‡ºå‘: {flight['departure_airport']} {flight['departure_time']}\n"
        message += f"ğŸ›¬ åˆ°è¾¾: {flight['arrival_airport']} {flight['arrival_time']}\n"
        message += f"â„¹ï¸ èˆªç­ä¿¡æ¯: {flight['FlightInformation']}\n"
        message += "-" * 30 + "\n"
    
    return message

def FlightsPage(departurePlace, arrivePlace, departureDate, headless=True):
    # é…ç½®æ— å¤´æµè§ˆå™¨
    if headless:
        co = ChromiumOptions()
        co.headless()  # è®¾ç½®æ— å¤´æ¨¡å¼
        page = WebPage(chromium_options=co)
    else:
        page = WebPage()
    
    page.get('https://flights.ctrip.com/online/list/oneway-' + departurePlace + '-' + arrivePlace + '?depdate='+ departureDate + '&cabin=y_s_c_f&adult=1&child=0&infant=0')
    # æ—¶é—´æ®µç­›é€‰
    page('#filter_item_time').click()
    sleep(1)
    page("@@u_key=filter_toggle_entry@@u_remark=ç‚¹å‡»ç­›é€‰é¡¹[FILTER_GROUP_TIME.DEPART/æ™šä¸Š 18~24ç‚¹]").click()
    sleep(1)
    # éšè—å…±äº«èˆªç­
    page('#filter_item_other').click()
    sleep(1)
    page("@@u_key=filter_toggle_entry@@u_remark=ç‚¹å‡»ç­›é€‰é¡¹[FILTER_GROUP_OTHER.HIDE_SHARED_FLIGHTS/éšè—å…±äº«èˆªç­]").click()
    for i in range(4):
        sleep(3)
        page.scroll.to_bottom()
    sleep(1)
    soup = BeautifulSoup(page.html, "html.parser")
    page.quit()  # å…³é—­æµè§ˆå™¨é‡Šæ”¾èµ„æº
    return soup

def AirlineNameDiv(FlightsDiv):
    return FlightsDiv.find_all("div",{"class":"airlineName"})

def AirlineNameDivFirst(FlightsDiv):
    return FlightsDiv.find_all("div",{"class":"airline-name"})

def GetDpDiv(FlightsDiv):
    return FlightsDiv.find("div",{"class":"depart-box"})

def GetArrDiv(FlightsDiv):
    return FlightsDiv.find("div",{"class":"arrive-box"})

def GetDpAirport(FlightsDiv):
    return GetDpDiv(FlightsDiv).find("div",{"class":"airport"}).text

def GetDpTime(FlightsDiv):
    return GetDpDiv(FlightsDiv).find("div",{"class":"time"}).text

def GetArrAirport(FlightsDiv):
    return GetArrDiv(FlightsDiv).find("div",{"class":"airport"}).text

def GetArrTime(FlightsDiv):
    return GetArrDiv(FlightsDiv).find("div",{"class":"time"}).text

def GetFlightInformation(FlightsDiv):
    return FlightsDiv.find("div",{"class":"transfer-info-group"}).text

def GetFlightPrice(FlightsDiv):
    return FlightsDiv.find("span",{"class":"price"}).text

def ReviseResult(FlightsDiv,airlineName):
    result={
    'airline': 'null',
    'departure_airport': 'null',
    'arrival_airport': 'null',
    'departure_time': 'null',
    'arrival_time': 'null',
    'FlightInformation':'null',
    'price': 'null'
    }
    result['airline'] = airlineName
    result['departure_airport']=GetDpAirport(FlightsDiv)
    result['arrival_airport']=GetArrAirport(FlightsDiv)
    result['departure_time']=GetDpTime(FlightsDiv)
    result['arrival_time']=GetArrTime(FlightsDiv)     
    result['FlightInformation']=GetFlightInformation(FlightsDiv)
    result['price']=GetFlightPrice(FlightsDiv)
    return result

def DataProcessing(FlightsDiv):   
    try:
        if len(AirlineNameDiv(FlightsDiv)) == 2:
            return ReviseResult(FlightsDiv, [AirlineNameDiv(FlightsDiv)[0].contents[0], AirlineNameDiv(FlightsDiv)[1].contents[0]])
        if len(AirlineNameDiv(FlightsDiv)) == 0:
            return ReviseResult(FlightsDiv, AirlineNameDivFirst(FlightsDiv)[0].text)
    except Exception as e:
        print(f"å¤„ç†èˆªç­æ•°æ®æ—¶å‡ºé”™: {e}")
        return None

def GetTopThreeFlights(allFlightsDiv):
    """è·å–ä»·æ ¼æœ€ä½çš„ä¸‰ä¸ªèˆªç­"""
    flights_data = []
    for FlightsDiv in allFlightsDiv[0:]:
        flight_info = DataProcessing(FlightsDiv)
        print(flight_info)
        if flight_info and flight_info['price'] != 'null':
            try:
                # æå–ä»·æ ¼æ•°å­—
                price_str = flight_info['price'].replace('Â¥', '').replace(',', '')
                flight_info['price_num'] = int(price_str)
                flights_data.append(flight_info)
            except ValueError:
                continue
    
    # æŒ‰ä»·æ ¼æ’åºï¼Œå–å‰ä¸‰ä¸ª
    flights_data.sort(key=lambda x: x['price_num'])
    return flights_data[:3]

def DisplayFlights(flights, webhook_url=None):
    """æ ¼å¼åŒ–æ˜¾ç¤ºèˆªç­ä¿¡æ¯å¹¶æ¨é€åˆ°é£ä¹¦"""
    print(f"\n=== æœ€ä½ä»·æ ¼èˆªç­ä¿¡æ¯ ({datetime.now().strftime('%Y-%m-%d %H:%M:%S')}) ===")
    print("-" * 80)
    
    for i, flight in enumerate(flights, 1):
        print(f"ç¬¬{i}å - ä»·æ ¼: {flight['price']}")
        print(f"  èˆªç©ºå…¬å¸: {flight['airline']}")
        print(f"  å‡ºå‘: {flight['departure_airport']} {flight['departure_time']}")
        print(f"  åˆ°è¾¾: {flight['arrival_airport']} {flight['arrival_time']}")
        print(f"  èˆªç­ä¿¡æ¯: {flight['FlightInformation']}")
        print("-" * 40)
    
    # æ¨é€åˆ°é£ä¹¦
    if webhook_url and flights:
        feishu_message = format_flights_for_feishu(flights)
        send_to_feishu(feishu_message, webhook_url)

def CrawlAndDisplay(webhook_url=None):
    """æŠ“å–å¹¶æ˜¾ç¤ºèˆªç­ä¿¡æ¯çš„ä¸»å‡½æ•°"""
    try:
        print("å¼€å§‹æŠ“å–èˆªç­ä¿¡æ¯...")
        soup = FlightsPage('sha', 'can', '2025-08-03', headless=False)
        all_flights = soup.find_all("div", {"class": "flight-box"})
        
        if len(all_flights) > 1:
            top_flights = GetTopThreeFlights(all_flights)
            if top_flights:
                DisplayFlights(top_flights, webhook_url)
            else:
                print("æœªæ‰¾åˆ°æœ‰æ•ˆçš„èˆªç­ä»·æ ¼ä¿¡æ¯")
                if webhook_url:
                    send_to_feishu("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„èˆªç­ä»·æ ¼ä¿¡æ¯", webhook_url)
        else:
            print("æœªæ‰¾åˆ°èˆªç­ä¿¡æ¯")
            if webhook_url:
                send_to_feishu("âš ï¸ æœªæ‰¾åˆ°èˆªç­ä¿¡æ¯", webhook_url)
            
    except Exception as e:
        error_msg = f"æŠ“å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}"
        print(error_msg)
        if webhook_url:
            send_to_feishu(f"âŒ {error_msg}", webhook_url)

def StartScheduledCrawling(interval_minutes=30, webhook_url=None):
    """å¯åŠ¨å®šæ—¶æŠ“å–ä»»åŠ¡ï¼ˆä½¿ç”¨å†…ç½®timeæ¨¡å—ï¼‰"""
    print(f"å¯åŠ¨å®šæ—¶æŠ“å–ä»»åŠ¡ï¼Œé—´éš”: {interval_minutes}åˆ†é’Ÿ")
    if webhook_url:
        print("âœ… å·²é…ç½®é£ä¹¦æ¨é€")
    
    while True:
        CrawlAndDisplay(webhook_url)
        print(f"ç­‰å¾… {interval_minutes} åˆ†é’Ÿåè¿›è¡Œä¸‹æ¬¡æŠ“å–...")
        time.sleep(interval_minutes * 60)  # è½¬æ¢ä¸ºç§’

if __name__ == "__main__":
    # é£ä¹¦æœºå™¨äºº webhook åœ°å€
    FEISHU_WEBHOOK = "https://open.feishu.cn/open-apis/bot/v2/hook/37da1252-cd91-4492-ac9b-d4f7fe56e6c3"
    
    # å¯åŠ¨å®šæ—¶æŠ“å–ï¼Œæ¯30åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡ï¼Œå¹¶æ¨é€åˆ°é£ä¹¦
    StartScheduledCrawling(15, FEISHU_WEBHOOK)
